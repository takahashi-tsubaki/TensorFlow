{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FriendRecommend.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYw+3COU4JTqC4DEdTtMNQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takahashi-tsubaki/TensorFlow/blob/friend_Recommend/FriendRecommend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m62-6xUIksdg"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA_URL = \"http://www.mrjyuryoku.com/model-Data.csv\"\n",
        "TEST_DATA_URL = \"http://www.mrjyuryoku.com/test-Data.csv\"\n",
        "\n",
        "train_file_path = tf.keras.utils.get_file(\"model-Data.csv\", TRAIN_DATA_URL)\n",
        "test_file_path = tf.keras.utils.get_file(\"test-Data.csv\", TEST_DATA_URL)"
      ],
      "metadata": {
        "id": "RNb27eFDkwZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy の値を読みやすくする\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "metadata": {
        "id": "9oiV4WmhmgNf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データのロード\n",
        "それではいつものように、扱っている CSV ファイルの先頭を見てみましょう。"
      ],
      "metadata": {
        "id": "mPgbb-a_1JPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head {train_file_path}"
      ],
      "metadata": {
        "id": "_prGrhZWmhFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pandasでデータを読み込んでから Numpy の array を TensorFlow に渡すというのは可能です。もし、大規模なデータに対応するためにスケールアップしたり、データの読み込み処理を TensorFlow や tf.data に統合されたものにしたい場合、tf.data.experimental.make_csv_dataset 関数が利用できます。"
      ],
      "metadata": {
        "id": "Y7ZmRclY1Np5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [0, 1, 2]\n",
        "LABEL_COLUMN =  'CategoryID'"
      ],
      "metadata": {
        "id": "9ljG0tvffK20"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "コンストラクタの引数の値が揃ったので、ファイルから CSV データを読み込みデータセットを作る。\n",
        "\n",
        "（完全なドキュメントは、tf.data.experimental.make_csv_dataset を参照してください）"
      ],
      "metadata": {
        "id": "y4jM4y7z1Foz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=5, # Artificially small to make examples easier to show.\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True, \n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "raw_train_data = get_dataset(train_file_path)\n",
        "raw_test_data = get_dataset(test_file_path)"
      ],
      "metadata": {
        "id": "q5-cgRMufaID"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))"
      ],
      "metadata": {
        "id": "TKsp7F1lfgQe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセットを構成する要素は、(複数のサンプル, 複数のラベル)の形のタプルとして表されるバッチです。サンプル中のデータは（行ベースのテンソルではなく）列ベースのテンソルとして構成され、それぞれはバッチサイズ（このケースでは5個）の要素が含まれます。\n",
        "\n",
        "実際に見てみましょう。"
      ],
      "metadata": {
        "id": "ECPM5dJm1Pyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_batch(raw_train_data)"
      ],
      "metadata": {
        "id": "OpYqDN-T1P6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上で見たように、この CSV の列には名前がついています。Dataset のコンストラクターはこれらの列名を自動的に抽出します。一行目に列名が記されていない CSV を扱う場合には、列名のリストを make_csv_dataset 関数の column_names 引数に渡してください。"
      ],
      "metadata": {
        "id": "OOzrj44Q1VSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_COLUMNS = ['DataID','CategoryID', 'Category']\n",
        "\n",
        "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
        "\n",
        "show_batch(temp_dataset)"
      ],
      "metadata": {
        "id": "7AqE6R251V_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このサンプルでは利用できるすべての列を使います。もし、データセットから特定の列を除外したい場合には、利用したい列名のリストを作成し、コンストラクタのオプショナルな引数である select_columns にそのリストを渡してください。"
      ],
      "metadata": {
        "id": "_XIftqJh6PD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT_COLUMNS = ['DataID','CategoryID', 'Category']\n",
        "\n",
        "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
        "\n",
        "show_batch(temp_dataset)"
      ],
      "metadata": {
        "id": "mlYaKV405-R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データの前処理"
      ],
      "metadata": {
        "id": "P1yEd5IQFpJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "CSV ファイルに含まれるデータには様々な型のものがありえます。データをモデルに入力する前に、典型的なケースでは、様々な型を含んだデータを固定長のベクトルに変換する必要があります。\n",
        "\n",
        "TensorFlow には典型的なデータ変換を行う tf.feature_column が組み込まれています。詳細はこのチュートリアルを参照してください。\n",
        "\n",
        "また、任意のツール (たとえば、nltk や sklearn) を使って前処理を行い、前処理した結果を TensorFlow にわたすこともできます。\n",
        "\n",
        "モデルの内部で前処理を行う最大の利点は、モデルをエクスポートしたときにその中に前処理が含まれていることです。この方法を取れば、生データをそのままモデルに入力することができます。"
      ],
      "metadata": {
        "id": "2-uc1-weFuOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "すでにデータが適切に数値型になっている場合、モデルに渡す前にそのデータをベクトル形式にできます。"
      ],
      "metadata": {
        "id": "9ibFm8kORbVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT_COLUMNS = ['DataID','CategoryID']\n",
        "DEFAULTS = [0,0.0]\n",
        "temp_dataset = get_dataset(train_file_path, \n",
        "                           select_columns=SELECT_COLUMNS,\n",
        "                           column_defaults = DEFAULTS)\n",
        "\n",
        "show_batch(temp_dataset)"
      ],
      "metadata": {
        "id": "mSfBbKwD3XLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch, labels_batch = next(iter(temp_dataset)) "
      ],
      "metadata": {
        "id": "Qcnc-WB73mcQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のサンプルは、数値型の列をベクトルに変換するシンプルな関数の例です。"
      ],
      "metadata": {
        "id": "_yXj9zMm3zfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9OciZri6Kdml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zyFueYbkKeA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pack(features, label):\n",
        "  return tf.stack(list(features.values()), axis=-1), label"
      ],
      "metadata": {
        "id": "6RQVHGNr3z80"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この関数をデータセットのそれぞれの要素に適用するする。"
      ],
      "metadata": {
        "id": "1p4oHlZp33XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packed_dataset = temp_dataset.map(pack)\n",
        "\n",
        "for features, labels in packed_dataset.take(1):\n",
        "  print(features.numpy())\n",
        "  print()\n",
        "  print(labels.numpy())"
      ],
      "metadata": {
        "id": "faGcseVE34F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "様々な型を含んだデータセットを利用する場合、シンプルな数値型のフィールドだけを分離したくなるかもしれません。tf.feature_column はそのような処理を実現できますが、いくらかのオーバーヘッドを発生させるため、それが本当に必要になる場合以外ではできるだけ避けるべきでしょう。様々な型を含んだデータセットに話を戻しましょう。"
      ],
      "metadata": {
        "id": "Huwjj3YU36jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_batch(raw_train_data)"
      ],
      "metadata": {
        "id": "6wkHF6sa37_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch, labels_batch = next(iter(temp_dataset))"
      ],
      "metadata": {
        "id": "rgvH_xOq39O1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数値型の特徴量を選んで、それらをベクトル化して単一の列に変換する。"
      ],
      "metadata": {
        "id": "8GnnRzEAT9Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PackNumericFeatures(object):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "iflIQ26OT9tp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMERIC_FEATURES = ['DataID']\n",
        "\n",
        "packed_train_data = raw_train_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "packed_test_data = raw_test_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))"
      ],
      "metadata": {
        "id": "l5a1qdoHUAbV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_batch(packed_train_data)"
      ],
      "metadata": {
        "id": "GKy4lw4wUAg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch, labels_batch = next(iter(packed_train_data))"
      ],
      "metadata": {
        "id": "WCJgCwtMUCiV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データの正規化"
      ],
      "metadata": {
        "id": "uBg_3zcmVGsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
        "desc"
      ],
      "metadata": {
        "id": "fZcNok1fVJwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEAN = np.array(desc.T['mean'])\n",
        "STD = np.array(desc.T['std'])"
      ],
      "metadata": {
        "id": "NQmOoeFgVS1K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_numeric_data(data, mean, std):\n",
        "  # Center the data\n",
        "  return (data-mean)/std"
      ],
      "metadata": {
        "id": "gp3qyQ0MVUOq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数値型の列を作成しましょう。tf.feature_columns.numeric_column では normalizer_fn 引数で正則化のための関数を受け取ります。また、渡された関数はそれぞれのバッチに対して実行されます。\n",
        "\n",
        "functools.partial を用いて関数を部分適用し、normalize_numeric_data の引数 MEAN と STD を固定しておきましょう。"
      ],
      "metadata": {
        "id": "c-wnqOTSVZo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See what you just created.\n",
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns = [numeric_column]\n",
        "numeric_column"
      ],
      "metadata": {
        "id": "jG-SKYVBVX64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの訓練時には、数値データのブロックを選び出して利用できるように、作成した特徴量の列を入力に含めましょう。"
      ],
      "metadata": {
        "id": "a5OoMHDIVa_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch['numeric']"
      ],
      "metadata": {
        "id": "QI0i7vJ8VeQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
        "numeric_layer(example_batch).numpy()"
      ],
      "metadata": {
        "id": "LoC3qDaZVgKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "カテゴリデータ\n",
        "この CSV データ中のいくつかの列はカテゴリ列です。つまり、その中身は、限られた選択肢の中のひとつである必要があります。\n",
        "\n",
        "tf.feature_column API を用いて、それぞれのカテゴリ列のためのコレクションを tf.feature_column.indicator_column で生成しましょう。"
      ],
      "metadata": {
        "id": "pJowt1eFVmYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORIES = {\n",
        "    'Category': ['cube', 'sphere','capsule'],\n",
        "   \n",
        "}"
      ],
      "metadata": {
        "id": "AqPjefiOVnir"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
      ],
      "metadata": {
        "id": "UD-X29l9V_ki"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See what you just created.\n",
        "categorical_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzPvxNzdWCTp",
        "outputId": "90f509fb-cd21-44ea-fb00-a1d52edd7d49"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='Category', vocabulary_list=('cube', 'sphere', 'capsule'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
        "print(categorical_layer(example_batch).numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLgiXBN_WCx1",
        "outputId": "8da574c0-4d7b-4d3c-c00b-3887a4ac5cee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルを構築する際、これは入力レイヤーで行われるデータ処理の一部です。"
      ],
      "metadata": {
        "id": "1ZWp-w76Wb83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 一体化した前処理レイヤー"
      ],
      "metadata": {
        "id": "eUiAG8osXAyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2つの特徴量列のコレクションを結合し、tf.keras.layers.DenseFeatures に入力して、両方のデータ型を読み込んで前処理する入力レイヤーを作成しましょう。"
      ],
      "metadata": {
        "id": "V8KOwZGTWh4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
      ],
      "metadata": {
        "id": "VWDDeLKGWsHY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessing_layer(example_batch).numpy()[0])"
      ],
      "metadata": {
        "id": "WKsEfrjzWtRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルの構築"
      ],
      "metadata": {
        "id": "yc7MjxeQW5St"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing_layer から始まる tf.keras.Sequential を構築するする。"
      ],
      "metadata": {
        "id": "ynyhB7fgXIWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ytJDjlRQXDhd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練、評価、そして予測"
      ],
      "metadata": {
        "id": "E923fXG-XLZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これでモデルをインスタンス化し、訓練することができるる。"
      ],
      "metadata": {
        "id": "pAGvdTjUXNDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = packed_train_data.shuffle(500)\n",
        "test_data = packed_test_data"
      ],
      "metadata": {
        "id": "0si65gJaXMWd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, epochs=5)"
      ],
      "metadata": {
        "id": "I7weePQ5XQFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "Az3uICWhXRvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_data)\n",
        "\n",
        "# 結果のいくつかを表示\n",
        "for prediction, cube in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
        "  print(\"Predicted Cube: {:.4%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"CUBE\" if bool(cube) else \"Capsule\"))"
      ],
      "metadata": {
        "id": "3FjKmkAIXS7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}